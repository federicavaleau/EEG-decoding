{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlQg6XcqYVyT"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import torch\n",
        "import argparse\n",
        "import easydict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pywt\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, BatchNormalization, Dropout, Lambda\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import Input, Conv2D, Activation, Add, AveragePooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import torch; torch.utils.backcompat.broadcast_warning.enabled = True\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.backends.cudnn as cudnn; cudnn.benchmark = True\n",
        "from scipy.fftpack import fft, rfft, fftfreq, irfft, ifft, rfftfreq\n",
        "from scipy import signal\n",
        "import importlib\n",
        "from torch.autograd import Variable\n",
        "import gc\n",
        "import scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index\n",
        "40 classi:\n",
        "- wavelet + resnet (tf)\n",
        "- data reduction + bilstm (tf)\n",
        "- bilstm (torch spampinato)\n",
        "- cnn (torch)\n",
        "  \n",
        "2 classi:\n",
        "- bilstm (tf)\n",
        "- cnn (torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5eCgxyOuaTC"
      },
      "source": [
        "Getting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POo4EabGYc0o",
        "outputId": "8d859765-cd86-46a0-e3d1-953a5138e79b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F7z2XFavYVyb"
      },
      "outputs": [],
      "source": [
        "# Importing the files from the data folder\n",
        "file_path = 'data/eeg_5_95_std.pth'\n",
        "#eeg5_95 = torch.load(file_path)\n",
        "splits_all = torch.load('drive/MyDrive/Tesi_Valeau/data/block_splits_by_image_all.pth')\n",
        "#splits_single = torch.load('data/block_splits_by_image_single.pth')\n",
        "eeg14_70 = torch.load('drive/MyDrive/Tesi_Valeau/data/eeg_14_70_std.pth')\n",
        "#eeg55_95 = torch.load('drive/MyDrive/Tesi_Valeau/data/eeg_55_95_std.pth')\n",
        "eeg_raw = torch.load('drive/MyDrive/Tesi_Valeau/data/eeg_signals_raw_with_mean_std.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nGDywAE7YVyf"
      },
      "outputs": [],
      "source": [
        "# We focus on the dataset filtered with notch filter and 14-71 Hz\n",
        "dataset14 = eeg14_70['dataset']\n",
        "labels14  = eeg14_70['labels']\n",
        "images14 = eeg14_70['images']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kfnOn5nYVyh",
        "outputId": "49db4188-5cde-4ea6-864d-08f31ff41c09"
      },
      "outputs": [],
      "source": [
        "# Now we perform some further data pre-processing for the 40 class classification\n",
        "datasettone = [dataset14[i]['eeg'] for i in range(len(dataset14))]\n",
        "dataset = []\n",
        "for el in datasettone:\n",
        "    dataset.append(el[:,40:490])\n",
        "print(len(dataset), datasettone[0].shape, dataset[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehD4SrI_uebU"
      },
      "source": [
        "Wavelet transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "-SToOsunYral",
        "outputId": "f1357c95-646c-47cb-83ae-11ef66329f55"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def create_wavelet_image(signal_tensor):\n",
        "    num_channels, time_length = signal_tensor.shape\n",
        "    #signal_array = signal_tensor#.detach().cpu().numpy()\n",
        "    # Sample the length of the signal to one third of the original\n",
        "    signal_array=scipy.signal.decimate(signal_tensor, 3, axis=1)\n",
        "\n",
        "    wavelet_images = []\n",
        "    scale = [2**(x/2) for x in range(33)] # option to use this scale instead of np.arange(1,33), to be fed into the pywt.cwt() function\n",
        "    for channel in range(num_channels):\n",
        "\n",
        "        # Perform wavelet transform for the current channel\n",
        "        coefficients, _ = pywt.cwt(signal_array[channel], np.arange(1,33), 'gaus2')\n",
        "        normalized_coefficients = (coefficients - np.min(coefficients)) / (np.max(coefficients) - np.min(coefficients))\n",
        "        wavelet_images.append(normalized_coefficients)\n",
        "\n",
        "    # Stacking wavelet images along a new dimension to create a multi-channel image\n",
        "    multi_channel_image = np.stack(wavelet_images, axis=0)\n",
        "    multi_channel_image_tensor = torch.tensor(multi_channel_image)\n",
        "    return multi_channel_image_tensor\n",
        "\n",
        "# As an exaple, we take the first signal of the dataset to see the wavelet\n",
        "signal_tensor = dataset[0]\n",
        "print(signal_tensor.shape, dataset[0].shape)\n",
        "# create multi-channel wavelet image for first sample\n",
        "wavelet_image = create_wavelet_image(signal_tensor)\n",
        "print(wavelet_image.size())\n",
        "# Plot wavelet image\n",
        "plt.imshow(wavelet_image[0])\n",
        "plt.colorbar(label='Normalized Amplitude')\n",
        "plt.title('Wavelet Transform Image (Channel 0)')\n",
        "plt.xlabel('Pixel')\n",
        "plt.ylabel('Pixel')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9RQR1htYxIb",
        "outputId": "f894b2bf-92f8-4002-bc0f-2fd529b231a0"
      },
      "outputs": [],
      "source": [
        "# Creating the wavelet dataset\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_id, prov_id = torch.utils.data.random_split(range(len(dataset)), [0.7, 0.3], generator=generator1)\n",
        "val_id, test_id = torch.utils.data.random_split(prov_id, [0.5, 0.5], generator=generator1)\n",
        "\n",
        "wave_dataset_train = []\n",
        "wave_dataset_val = []\n",
        "wave_dataset_test = []\n",
        "data_train = [dataset[i] for i in train_id]\n",
        "data_val = [dataset[i] for i in val_id]\n",
        "data_test = [dataset[i] for i in test_id]\n",
        "\n",
        "print('done')\n",
        "for x in range(len(data_train)):\n",
        "    wave_dataset_train.append(create_wavelet_image(data_train[x]))\n",
        "#    if x%10 == 0:\n",
        "#        print(x)\n",
        "\n",
        "for x in range(len(data_val)):\n",
        "    wave_dataset_val.append(create_wavelet_image(data_val[x]))\n",
        "\n",
        "for x in range(len(data_test)):\n",
        "    wave_dataset_test.append(create_wavelet_image(data_test[x]))\n",
        "print(len(wave_dataset_train), wave_dataset_train[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWzxp6y4rYPn",
        "outputId": "f02deb2c-e74c-42c5-eeec-94980d076ec1"
      },
      "outputs": [],
      "source": [
        "# Getting the labels and changing everything to be a numpy array\n",
        "y_train = np.array([dataset14[i]['label'] for i in train_id])\n",
        "y_val = np.array([dataset14[i]['label'] for i in val_id])\n",
        "y_test = np.array([dataset14[i]['label'] for i in test_id])\n",
        "print(len(y_train))\n",
        "\n",
        "X_train_w = np.array([tensor.numpy() for tensor in wave_dataset_train])\n",
        "X_val_w = np.array([tensor.numpy() for tensor in wave_dataset_val])\n",
        "X_test_w = np.array([tensor.numpy() for tensor in wave_dataset_test])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0DdpTcSujRU"
      },
      "source": [
        "CNN with skip connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImPcSBRdqAdC",
        "outputId": "fc134af8-a232-4652-afdb-edc2eff2a20e"
      },
      "outputs": [],
      "source": [
        "# CNN with skip connection model\n",
        "def convolutional_block(x, filters, strides=(1, 1)):\n",
        "    shortcut = x\n",
        "\n",
        "    # first convolutional layer\n",
        "    x = Conv2D(filters, (3, 3), strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # second convolutional layer\n",
        "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # skip connection\n",
        "    shortcut = Conv2D(filters, (1, 1), strides=strides, padding='same')(shortcut)\n",
        "    shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    for _ in range(6):\n",
        "        x = convolutional_block(x, filters)\n",
        "    return x\n",
        "\n",
        "input_layer = Input(shape=(128, 32, 150))\n",
        "\n",
        "# First convolutional block\n",
        "x = Conv2D(64, (7, 7), strides=(2, 2), padding='same')(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "# Residual blocks\n",
        "x = residual_block(x, 64)\n",
        "#x = residual_block(x, 128)\n",
        "#x = residual_block(x, 256)\n",
        "\n",
        "# Final layers\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "output_layer_w = Dense(40, activation='softmax')(x)\n",
        "\n",
        "model_w = Model(inputs=input_layer, outputs=output_layer_w)\n",
        "model_w.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Kch91405rDbE"
      },
      "outputs": [],
      "source": [
        "# for saving metrica\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.train_losses.append(logs['loss'])\n",
        "        self.train_accuracies.append(logs['accuracy'])\n",
        "        self.val_losses.append(logs['val_loss'])\n",
        "        self.val_accuracies.append(logs['val_accuracy'])\n",
        "\n",
        "metrics_callback = MetricsCallback()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7zoWRtoqfx-",
        "outputId": "845575b6-ce57-4c69-b671-cddfd2bed385"
      },
      "outputs": [],
      "source": [
        "# Here we compile and train the model\n",
        "metrics2 = MetricsCallback()\n",
        "\n",
        "\n",
        "model_w.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_w.fit(X_train_w, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=200,\n",
        "                    validation_data=(X_val_w, y_val), callbacks=[metrics2], verbose = 1)\n",
        "\n",
        "\n",
        "loss_w, accuracy_w = model_w.evaluate(X_test_w, y_test)\n",
        "print(\"Test Loss:\", loss_w)\n",
        "print(\"Test Accuracy:\", accuracy_w)\n",
        "\n",
        "# save training and validation metrics to text file\n",
        "with open('metrics2.txt', 'w') as f:\n",
        "    f.write(\"Epoch\\tTrain Loss\\tTrain Accuracy\\tVal Loss\\tVal Accuracy\\n\")\n",
        "    for epoch, (train_loss, train_accuracy, val_loss, val_accuracy) in enumerate(zip(metrics_callback.train_losses,\n",
        "                                                                                      metrics_callback.train_accuracies,\n",
        "                                                                                      metrics_callback.val_losses,\n",
        "                                                                                      metrics_callback.val_accuracies)):\n",
        "        f.write(f\"{epoch+1}\\t{train_loss}\\t{train_accuracy}\\t{val_loss}\\t{val_accuracy}\\n\")\n",
        "\n",
        "with open('metrics.txt', 'a') as f:\n",
        "    f.write(f\"\\nTest Loss: {loss_w}\\n\")\n",
        "    f.write(f\"Test Accuracy: {accuracy_w}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1t_6piNuMsQ"
      },
      "source": [
        "#### Data dimension reduction\n",
        "\n",
        "What we do here is chaning the shape of the input signals: from a 128x500 tensor, each input becomes a 60x8 tensor. We do so by dividing the electrodos in three sections: middle (8 electrodes), right and left (60 electrodes each). They we combine the information in this way:\n",
        "$$\n",
        " C = [c_i]_{i=1}^{128}  \\\\\n",
        " C^m = [c_i]_i^{l_{ch}} \\\\\n",
        " d_j = c_j^l-c_j^r, \\ j = 1 ... l_{ch}, \\\\ D = [d_j]_{j = 1}^{l_{ch}} \\\\\n",
        " S = [D \\ C^{m^T}]\n",
        "$$\n",
        "\n",
        "Below, we do what we just explained, with the correct indices and electrode configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBc6P40buHei"
      },
      "outputs": [],
      "source": [
        "# Channel mapping (second version)\n",
        "#1-32 green channels, 33-64 yellow channels, 65-96 red channels, 97-128 white channels\n",
        "channel_map = {\n",
        "    # Green channels\n",
        "    1:'Fp1', 2:'Fp2', 3: 'F7', 4:'F3', 5:'Fz', 6:'F4', 7:'F8', 8:'FC5', 9:'FC1',\n",
        "    10:'FC2', 11:'FC6', 12:'T7', 13:'C3', 14:'Cz', 15: 'C4', 16: 'T8', 17: 'TP9',\n",
        "    18:'CP5', 19:'CP1', 20:'CP2', 21:'CP6', 22:'TP10', 23:'P7', 24:'P3', 25:'Pz',\n",
        "    26:'P4', 27:'P8', 28:'PO9', 29:'O1', 30:'Oz', 31:'O2', 32:'P010',\n",
        "    # Yellow channels\n",
        "    33:'AF7', 34:'AF3', 35:'AF4', 36:'AF8', 37:'F5', 38:'F1', 39:'F2', 40:'F6',\n",
        "    41:'FT9', 42:'FT7', 43:'FC3', 44:'FC4', 45:'FT8', 46:'FT10', 47:'C5', 48:'C1',\n",
        "    49:'C2', 50:'C6', 51:'TP7', 52:'CP3', 53:'CPz', 54:'CP4', 55:'TP8', 56:'P5',\n",
        "    57:'P1', 58:'P2', 59:'P6', 60:'PO7', 61:'PO3', 62:'POz', 63:'PO4', 64:'PO8',\n",
        "    # Red channels\n",
        "    65: 'Fpz', 66: 'F9', 67: 'AFF5h', 68: 'AFF1h', 69: 'AFF2h', 70: 'AFF6h',\n",
        "    71: 'F10', 72: 'FTT9h', 73: 'FTT7h', 74: 'FCC5h', 75: 'FCC3h', 76: 'FCC1h',\n",
        "    77: 'FCC2h', 78: 'FCC4h', 79: 'FCC6h', 80: 'FTT8h', 81: 'FTT10h', 82: 'TPP9h',\n",
        "    83: 'TPP7h', 84: 'CPP5h', 85: 'CPP3h', 86: 'CPP1h', 87: 'CPP2h', 88: 'CPP4h',\n",
        "    89: 'CPP6h', 90: 'TPP8h', 91: 'TPP10h', 92: 'POO9h', 93: 'POO1', 94: 'POO2',\n",
        "    95: 'POO10h', 96: 'Iz',\n",
        "    # White channels\n",
        "    97: 'AFp1', 98: 'AFp2', 99: 'FFT9h', 100: 'FFT7h', 101: 'FFC5h', 102: 'FFC3h',\n",
        "    103: 'FFC1h', 104: 'FFC2h', 105: 'FFC4h', 106: 'FFC6h', 107: 'FFT8h', 108: 'FFT10h',\n",
        "    109: 'TTP7h', 110: 'CCP5h', 111: 'CCP3h', 112: 'CCP1h', 113: 'CCP2h', 114:'CCP4h',\n",
        "    115: 'CCP6h', 116: 'TTP8h', 117: 'P9', 118: 'PPO9h', 119: 'PPO5h', 120: 'PPO1h',\n",
        "    121: 'PPO2h', 122: 'PPO6h', 123: 'PPO10h', 124: 'P10', 125: 'I1', 126: 'OI1h',\n",
        "    127: 'OI2h', 128:'I2'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxk5GMJtuwvK",
        "outputId": "87541f5f-50c1-4843-d39b-f8a1d014c3d6"
      },
      "outputs": [],
      "source": [
        "# middle indexes\n",
        "m = [64, 4, 13, 52,24,61,29,95]\n",
        "# totality of green electrodes, then divided in right and left parts\n",
        "lg = [1,3,4,8,9,12,13,17,18,19,23,24,28,29]\n",
        "rg = [2,7,6,11,10,16,15,22,21,10,27,26,32,31]\n",
        "lg = [x-1 for x in lg]\n",
        "rg = [x-1 for x in rg]\n",
        "# Same for yellow electrodes\n",
        "ly = [1,2,5,6,9,10,11,15,16,19,20,24,25,28,29]\n",
        "ry = [4,3,8,7,14,13,12,18,17,23,22,27,26,32,31]\n",
        "ly =[x +31 for x in ly ]\n",
        "ry =[x +31 for x in ry ]\n",
        "# red channels\n",
        "lr= [2,3,4,8,9,10,11,12,18,19,20,21,22,28,29]\n",
        "rr =[7,6,5,17,16,15,14,13,27,26,25,24,23,31,30]\n",
        "lr =[x +63 for x in lr ]\n",
        "rr =[x +63 for x in rr ]\n",
        "# white channels\n",
        "lw=[1,3,4,5,6,7,13,14,15,16,21,22,23,24,29,30]\n",
        "rw=[2,12,11,10,9,8,20,19,18,17,28,27,26,25,32,31]\n",
        "lw =[x +95 for x in lw ]\n",
        "rw =[x +95 for x in rw ]\n",
        "print(len(lw), len(rw))\n",
        "# Ordered indexes of right and left electrodes\n",
        "l = lg + ly + lr+lw\n",
        "r = rg +ry + rr+rw\n",
        "print(max(r), max(l))\n",
        "\n",
        "# creating a new dataset with reduced dimensionlity\n",
        "reduced_data = []\n",
        "for el in dataset:\n",
        "    m_ch = el[m , :]\n",
        "    d_ch = el[l,:]-el[r,:]\n",
        "    S = torch.mm(d_ch, m_ch.t())\n",
        "    reduced_data.append(S)\n",
        "\n",
        "\n",
        "print(len(reduced_data), reduced_data[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3EPOpXumwczY"
      },
      "outputs": [],
      "source": [
        "# ID Split for reduced dataset\n",
        "labels = [dataset14[i]['label'] for i in range(len(dataset14))]\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_id, val_id, test_id = torch.utils.data.random_split(range(len(dataset)), [0.7, 0.15, 0.15], generator=generator1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZCpuvGzwuah",
        "outputId": "eea34065-2e19-4577-9ace-8204d34e88ab"
      },
      "outputs": [],
      "source": [
        "# Dataset split for LSTM\n",
        "X_train = [reduced_data[i] for i in train_id]\n",
        "X_val = [reduced_data[i] for i in val_id]\n",
        "X_test = [reduced_data[i] for i in test_id]\n",
        "y_train = np.array([labels[i] for i in train_id])\n",
        "y_val = np.array([labels[i] for i in val_id])\n",
        "y_test = np.array([labels[i] for i in test_id])\n",
        "\n",
        "# Convert the list of torch tensors to a numpy array\n",
        "X_train = np.array([tensor.numpy() for tensor in X_train])\n",
        "X_val = np.array([tensor.numpy() for tensor in X_val])\n",
        "X_test = np.array([tensor.numpy() for tensor in X_test])\n",
        "\n",
        "print(len(X_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZLcCqxQ4gAe",
        "outputId": "7ffce358-94dc-44cd-e003-3f74462e36d9"
      },
      "outputs": [],
      "source": [
        "# Bi-LSTM model for dimensionally reduced data\n",
        "input_shape = (60,8)\n",
        "input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "def bi_lstm_block(x, units, return_sequences=True):\n",
        "    x = Bidirectional(LSTM(units, return_sequences = return_sequences))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return x\n",
        "\n",
        "#LSTM blocks\n",
        "x = bi_lstm_block(input_layer, 128)\n",
        "x = bi_lstm_block(x, 128)\n",
        "#x = bi_lstm_block(x, 128)\n",
        "#x = bi_lstm_block(x, 128)\n",
        "x = bi_lstm_block(x, 128, return_sequences=False)\n",
        "\n",
        "\n",
        "# Dense blocks\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "output_layer = Dense(40, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=input_layer, outputs = output_layer)\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qz5jDEEF6Fyi"
      },
      "outputs": [],
      "source": [
        "# for saving metrica\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.train_losses.append(logs['loss'])\n",
        "        self.train_accuracies.append(logs['accuracy'])\n",
        "        self.val_losses.append(logs['val_loss'])\n",
        "        self.val_accuracies.append(logs['val_accuracy'])\n",
        "\n",
        "metrics_callback = MetricsCallback()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Dci4fZ6THm",
        "outputId": "e4223725-ccab-4c30-abed-bc9b14fe2bdd"
      },
      "outputs": [],
      "source": [
        "# Training of the Bi-LSTM model\n",
        "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=250,\n",
        "                    validation_data=(X_val, y_val), callbacks=[metrics_callback],verbose=1)\n",
        "\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# save training and validation metrics to text file\n",
        "with open('metrics.txt', 'w') as f:\n",
        "    f.write(\"Epoch\\tTrain Loss\\tTrain Accuracy\\tVal Loss\\tVal Accuracy\\n\")\n",
        "    for epoch, (train_loss, train_accuracy, val_loss, val_accuracy) in enumerate(zip(metrics_callback.train_losses,\n",
        "                                                                                      metrics_callback.train_accuracies,\n",
        "                                                                                      metrics_callback.val_losses,\n",
        "                                                                                      metrics_callback.val_accuracies)):\n",
        "        f.write(f\"{epoch+1}\\t{train_loss}\\t{train_accuracy}\\t{val_loss}\\t{val_accuracy}\\n\")\n",
        "\n",
        "# save test metrics\n",
        "with open('metrics.txt', 'a') as f:\n",
        "    f.write(f\"\\nTest Loss: {test_loss}\\n\")\n",
        "    f.write(f\"Test Accuracy: {test_accuracy}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIi090FzOhU6"
      },
      "source": [
        "Full dataset without dimension reduction.  Bi-LSTM model implemented with PyTorch, using Percive Lab code base (da runnare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rDCE2xvYL8BZ"
      },
      "outputs": [],
      "source": [
        "# Defying all the paramethers\n",
        "opt = easydict.EasyDict({\n",
        "    \"eeg_dataset\": r\"drive/MyDrive/Tesi_Valeau/data/eeg_14_70_std.pth\",\n",
        "    \"splits_path\": r\"drive/MyDrive/Tesi_Valeau/data/block_splits_by_image_all.pth\",\n",
        "    \"split_num\" : 0,\n",
        "    \"subject\": 0,\n",
        "    \"time_low\": 20,\n",
        "    \"time_high\":460,\n",
        "    \"model_type\":'bilstm',\n",
        "    \"model_params\":'',\n",
        "    \"pretrained_net\":'',\n",
        "    \"batch_size\":512,\n",
        "    \"optim\":\"Adam\",\n",
        "    \"learning_rate\":0.001,\n",
        "    \"learning_rate_decay_by\":0.5,\n",
        "    \"learning_rate_decay_every\":10,\n",
        "    \"data-workers\":4,\n",
        "    \"epochs\":1700,\n",
        "    \"saveCheck\":200,\n",
        "    \"no_cuda\":False})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gRnT26M3NkcD"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "class EEGDataset:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, eeg_signals_path):\n",
        "        # Load EEG signals\n",
        "        loaded = torch.load(eeg_signals_path)\n",
        "        if opt.subject!=0:\n",
        "            self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
        "        else:\n",
        "            self.data=loaded['dataset']\n",
        "        self.labels = loaded[\"labels\"]\n",
        "        self.images = loaded[\"images\"]\n",
        "\n",
        "        # Compute size\n",
        "        self.size = len(self.data)\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Process EEG\n",
        "        eeg = self.data[i][\"eeg\"].float().t()\n",
        "        eeg = eeg[opt.time_low:opt.time_high,:]\n",
        "\n",
        "        if opt.model_type == \"model10\":\n",
        "            eeg = eeg.t()\n",
        "            eeg = eeg.view(1,128,opt.time_high-opt.time_low)\n",
        "        # Get label\n",
        "        label = self.data[i][\"label\"]\n",
        "        # Return\n",
        "        return eeg, label\n",
        "\n",
        "# Splitter class\n",
        "class Splitter:\n",
        "\n",
        "    def __init__(self, dataset, split_path, split_num=0, split_name=\"train\"):\n",
        "        # Set EEG dataset\n",
        "        self.dataset = dataset\n",
        "        # Load split\n",
        "        loaded = torch.load(split_path)\n",
        "        self.split_idx = loaded[\"splits\"][split_num][split_name]\n",
        "        # Filter data\n",
        "        self.split_idx = [i for i in self.split_idx if 450 <= self.dataset.data[i][\"eeg\"].size(1) <= 600]\n",
        "        # Compute size\n",
        "        self.size = len(self.split_idx)\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Get sample from dataset\n",
        "        eeg, label = self.dataset[self.split_idx[i]]\n",
        "        # Return\n",
        "        return eeg, label\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = EEGDataset(opt.eeg_dataset)\n",
        "# Create loaders\n",
        "loaders = {split: DataLoader(Splitter(dataset, split_path = opt.splits_path, split_num = opt.split_num, split_name = split), batch_size = opt.batch_size, drop_last = True, shuffle = True) for split in [\"train\", \"val\", \"test\"]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWKSfibQORhB",
        "outputId": "7b45996c-8c8a-4250-eb11-cafa49da6e45"
      },
      "outputs": [],
      "source": [
        "# New implementation of Bi-LSTM network on pytorch\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=128, hidden_sizes=[128,128, 128,128,64], dense_sizes=[64,32], output_size=128, dropout_probs=[0.5,0.3]):\n",
        "        super().__init__()\n",
        "\n",
        "        # LSTM blocks\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, bidirectional=True)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(hidden_sizes[0]*2)\n",
        "        self.dropout1 = nn.Dropout(dropout_probs[0])\n",
        "\n",
        "        self.lstm2 = nn.LSTM(hidden_sizes[0]*2, hidden_sizes[1], batch_first=True, bidirectional=True)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(hidden_sizes[1]*2)\n",
        "        self.dropout2 = nn.Dropout(dropout_probs[0])\n",
        "\n",
        "        self.lstm3 = nn.LSTM(hidden_sizes[1]*2, hidden_sizes[2], batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.batchnorm3 = nn.BatchNorm1d(hidden_sizes[2]*2)\n",
        "        self.dropout3 = nn.Dropout(dropout_probs[0])\n",
        "\n",
        "        self.lstm4 = nn.LSTM(hidden_sizes[2]*2 , hidden_sizes[3], batch_first=True, bidirectional=True)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(hidden_sizes[3] *2)\n",
        "        self.dropout4 = nn.Dropout(dropout_probs[0])\n",
        "\n",
        "        self.lstm5 = nn.LSTM(hidden_sizes[3]*2 , hidden_sizes[4], batch_first=True, bidirectional=True)\n",
        "\n",
        "\n",
        "        # Dense blocks\n",
        "        # To be noted: when changing the number of layers remember to change hidden_sizes index\n",
        "        self.dense1 = nn.Linear(hidden_sizes[4]*2 , dense_sizes[0])\n",
        "        self.batchnorm5 = nn.BatchNorm1d(dense_sizes[0])\n",
        "        self.dropout5 = nn.Dropout(dropout_probs[1])\n",
        "\n",
        "        self.dense2 = nn.Linear(dense_sizes[0], dense_sizes[1])\n",
        "        self.batchnorm6 = nn.BatchNorm1d(dense_sizes[1])\n",
        "        self.dropout6 = nn.Dropout(dropout_probs[1])\n",
        "\n",
        "\n",
        "        self.output = nn.Linear(dense_sizes[1], output_size)\n",
        "        self.classifier = nn.Linear(output_size, 40)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Prepare LSTM initial states\n",
        "        batch_size = x.size(0)\n",
        "        def init_lstm_state(hidden_size):\n",
        "            h_0 = torch.zeros(2, batch_size, hidden_size)  # 2 for bidirectional\n",
        "            c_0 = torch.zeros(2, batch_size, hidden_size)\n",
        "            if x.is_cuda:\n",
        "                h_0, c_0 = h_0.cuda(), c_0.cuda()\n",
        "            return (Variable(h_0), Variable(c_0))\n",
        "\n",
        "        lstm_init1 = init_lstm_state(self.lstm1.hidden_size)\n",
        "        lstm_init2 = init_lstm_state(self.lstm2.hidden_size)\n",
        "        lstm_init3 = init_lstm_state(self.lstm3.hidden_size)\n",
        "\n",
        "        lstm_init4 = init_lstm_state(self.lstm4.hidden_size)\n",
        "        lstm_init5 = init_lstm_state(self.lstm5.hidden_size)\n",
        "\n",
        "        # Forward through LSTM layers\n",
        "        #print(x.shape, type(x),x )\n",
        "        x, _ = self.lstm1(x, lstm_init1)\n",
        "        x = self.batchnorm1(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x, _ = self.lstm2(x, lstm_init2)\n",
        "        x = self.batchnorm2(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x, _ = self.lstm3(x, lstm_init3)\n",
        "        x = self.batchnorm3(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x, _ = self.lstm4(x, lstm_init4)\n",
        "        x = self.batchnorm4(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x, _ = self.lstm5(x, lstm_init5)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Forward through Dense layers\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = self.batchnorm5(x)\n",
        "        x = self.dropout5(x)\n",
        "\n",
        "        x = F.relu(self.dense2(x))\n",
        "        x = self.batchnorm6(x)\n",
        "        x = self.dropout6(x)\n",
        "\n",
        "        x = F.relu(self.output(x))\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXWkfi7uOdEO"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "\n",
        "# Empty cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "# importing extra parameters\n",
        "model_options = {key: int(value) if value.isdigit() else (float(value) if value[0].isdigit() else value) for (key, value) in [x.split(\"=\") for x in opt.model_params]}\n",
        "# Create discriminator model/optimizer\n",
        "model = Model(**model_options)\n",
        "optimizer = getattr(torch.optim, opt.optim)(model.parameters(), lr = opt.learning_rate, weight_decay = 1e-5)\n",
        "\n",
        "# Setup CUDA\n",
        "if not opt.no_cuda:\n",
        "    model.cuda()\n",
        "    print(\"Copied to CUDA\")\n",
        "\n",
        "if opt.pretrained_net != '':\n",
        "        model = torch.load(opt.pretrained_net)\n",
        "        print(model)\n",
        "\n",
        "# initialize training,validation, test losses and accuracy list\n",
        "losses_per_epoch={\"train\":[], \"val\":[],\"test\":[]}\n",
        "accuracies_per_epoch={\"train\":[],\"val\":[],\"test\":[]}\n",
        "\n",
        "best_accuracy = 0\n",
        "best_accuracy_val = 0\n",
        "best_epoch = 0\n",
        "# Start training\n",
        "\n",
        "predicted_labels = []\n",
        "correct_labels = []\n",
        "\n",
        "for epoch in range(1, opt.epochs+1):\n",
        "    # Initialize loss/accuracy variables\n",
        "    losses = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "    accuracies = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "    counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "    # Adjust learning rate for SGD\n",
        "    if opt.optim == \"SGD\":\n",
        "        lr = opt.learning_rate * (opt.learning_rate_decay_by ** (epoch // opt.learning_rate_decay_every))\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "    # Process each split\n",
        "    for split in (\"train\", \"val\", \"test\"):\n",
        "        # Set network mode\n",
        "        if split == \"train\":\n",
        "            model.train()\n",
        "            torch.set_grad_enabled(True)\n",
        "        else:\n",
        "            model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "        # Process all split batches\n",
        "        for i, (input, target) in enumerate(loaders[split]):\n",
        "            # Check CUDA\n",
        "            if not opt.no_cuda:\n",
        "                input = input.to(\"cuda\")\n",
        "                target = target.to(\"cuda\")\n",
        "            # Forward\n",
        "            output = model(input)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            losses[split] += loss.item()\n",
        "            # Compute accuracy\n",
        "            _,pred = output.data.max(1)\n",
        "            correct = pred.eq(target.data).sum().item()\n",
        "            accuracy = correct/input.data.size(0)\n",
        "            accuracies[split] += accuracy\n",
        "            counts[split] += 1\n",
        "            # Backward and optimize\n",
        "            if split == \"train\":\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    # Print info at the end of the epoch\n",
        "    if accuracies[\"val\"]/counts[\"val\"] >= best_accuracy_val:\n",
        "        best_accuracy_val = accuracies[\"val\"]/counts[\"val\"]\n",
        "        best_accuracy = accuracies[\"test\"]/counts[\"test\"]\n",
        "        best_epoch = epoch\n",
        "\n",
        "    TrL,TrA,VL,VA,TeL,TeA=  losses[\"train\"]/counts[\"train\"],accuracies[\"train\"]/counts[\"train\"],losses[\"val\"]/counts[\"val\"],accuracies[\"val\"]/counts[\"val\"],losses[\"test\"]/counts[\"test\"],accuracies[\"test\"]/counts[\"test\"]\n",
        "    print(\"Model: {11} - Subject {12} - Time interval: [{9}-{10}]  [{9}-{10} Hz] - Epoch {0}: TrL={1:.4f}, TrA={2:.4f}, VL={3:.4f}, VA={4:.4f}, TeL={5:.4f}, TeA={6:.4f}, TeA at max VA = {7:.4f} at epoch {8:d}\".format(epoch,\n",
        "                                                                                                         losses[\"train\"]/counts[\"train\"],\n",
        "                                                                                                         accuracies[\"train\"]/counts[\"train\"],\n",
        "                                                                                                         losses[\"val\"]/counts[\"val\"],\n",
        "                                                                                                         accuracies[\"val\"]/counts[\"val\"],\n",
        "                                                                                                         losses[\"test\"]/counts[\"test\"],\n",
        "                                                                                                         accuracies[\"test\"]/counts[\"test\"],\n",
        "                                                                                                         best_accuracy, best_epoch, opt.time_low,opt.time_high, opt.model_type,opt.subject))\n",
        "\n",
        "    losses_per_epoch['train'].append(TrL)\n",
        "    losses_per_epoch['val'].append(VL)\n",
        "    losses_per_epoch['test'].append(TeL)\n",
        "    accuracies_per_epoch['train'].append(TrA)\n",
        "    accuracies_per_epoch['val'].append(VA)\n",
        "    accuracies_per_epoch['test'].append(TeA)\n",
        "\n",
        "    if epoch%opt.saveCheck == 0:\n",
        "                torch.save(model, '%s__subject%d_epoch_%d.pth' % (opt.model_type, opt.subject,epoch))\n",
        "\n",
        "print(f\"TrL: {min((val, id+1) for id, val in enumerate(losses_per_epoch['train']))}\")\n",
        "print(f\"VL: {min((val, id+1) for id, val in enumerate(losses_per_epoch['val']))}\")\n",
        "print(f\"TeL: {min((val, id+1) for id, val in enumerate(losses_per_epoch['test']))}\")\n",
        "print(f\"TrV: {max((val, id+1) for id, val in enumerate(accuracies_per_epoch['train']))}\")\n",
        "print(f\"VA: {max((val, id+1) for id, val in enumerate(accuracies_per_epoch['val']))}\")\n",
        "print(f\"TeA: {max((val, id+1) for id, val in enumerate(accuracies_per_epoch['test']))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMsy7SJ-PKms"
      },
      "source": [
        "CNN with PyTorch for 40 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfnEMTiGNCZz",
        "outputId": "c084a220-353e-497d-af2a-e0b7bcf274e3"
      },
      "outputs": [],
      "source": [
        "# Adjust the dataset for 40 class CNN\n",
        "datasettone = [dataset14[i]['eeg'] for i in range(len(dataset14))]\n",
        "dataset = []\n",
        "for el in datasettone:\n",
        "    dataset.append(el[:,40:490])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AftX81GrLeYI",
        "outputId": "5bd01140-375c-496e-8536-1802eb55a0ff"
      },
      "outputs": [],
      "source": [
        "# ID Split for 40 class CNN\n",
        "labels = [dataset14[i]['label'] for i in range(len(dataset14))]\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_id, val_id = torch.utils.data.random_split(range(len(dataset)), [0.8, 0.2], generator=generator1)\n",
        "\n",
        "# Dataset split for CNN\n",
        "X_train = torch.stack([dataset[i] for i in train_id])\n",
        "X_val = torch.stack([dataset[i] for i in val_id])\n",
        "y_train = torch.tensor([labels[i] for i in train_id])\n",
        "y_val = torch.tensor([labels[i] for i in val_id])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Yhhj1pRWPOju",
        "outputId": "6122ab12-5306-41fb-b76d-e9dd9ebeb2b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.flatten_size = self._get_flatten_size()\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 1024, bias=True)\n",
        "        self.fc2 = nn.Linear(1024, 40, bias=True)\n",
        "\n",
        "    def _get_flatten_size(self):\n",
        "        x = torch.randn(1, 128, 450)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = self.pool(torch.relu(self.conv4(x)))\n",
        "        return x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Moving the dataset to the GPU\n",
        "x_train = X_train.to(device=\"cuda\")\n",
        "x_val = X_val.to(device=\"cuda\")\n",
        "y_train = y_train.to(device='cuda')\n",
        "y_val = y_val.to(device='cuda')\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device='cuda')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels=inputs.to(device=\"cuda\"), labels.to(device=\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_labels in val_loader:\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_labels).item() * val_inputs.size(0)\n",
        "            _, val_predicted = torch.max(val_outputs, 1)\n",
        "            val_total += val_labels.size(0)\n",
        "            val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    # Print results for the current epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz8WiDCIPQAF"
      },
      "source": [
        "Performing the binary split to create a new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnXGGqzTPSZb",
        "outputId": "e10e0ecd-6702-4e44-bbdf-1e153100c09b"
      },
      "outputs": [],
      "source": [
        "# Using the mapping from class codes to class names\n",
        "file = open(\"mapping.txt\", \"r\")\n",
        "content = file.readlines()\n",
        "book = []\n",
        "for el in content:\n",
        "    book.append(el.split())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNQX5M8gPlLN",
        "outputId": "c4cdea31-cb2f-4831-865d-3c45fad01b06"
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary for the classes\n",
        "dic = {}\n",
        "for i in range(len(labels14)):\n",
        "    for el in book:\n",
        "        if labels14[i] == el[0]:\n",
        "            dic[i] = el[2]\n",
        "\n",
        "print(dic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqB-Tk_aPn1I",
        "outputId": "168c1244-2cb1-4fdb-9b40-535f46a517fc"
      },
      "outputs": [],
      "source": [
        "# Now creating the new dataset containing the classes objects and animals\n",
        "obj = [5,36, 18, 15, 6]\n",
        "animal = [8, 12, 21, 35, 39]\n",
        "\n",
        "# First objects\n",
        "obj_id = []\n",
        "for i in range(len(dataset14)):\n",
        "    if dataset14[i]['label'] in obj:\n",
        "        obj_id.append(i)\n",
        "data_obj = [dataset14[i]['eeg'] for i in obj_id]\n",
        "\n",
        "print(len(data_obj))\n",
        "\n",
        "# Then animals\n",
        "an_id = []\n",
        "for i in range(len(dataset14)):\n",
        "    if dataset14[i]['label'] in animal:\n",
        "        an_id.append(i)\n",
        "data_an = [dataset14[i]['eeg'] for i in an_id]\n",
        "\n",
        "print(len(data_obj), len(data_an))\n",
        "# Combining the classes\n",
        "dataset_bin_prov = data_obj + data_an\n",
        "labels_bin = [0 for x in range(1494)]+[1 for x in range(1500)]\n",
        "dataset_bin = []\n",
        "for el in dataset_bin_prov:\n",
        "    dataset_bin.append(el[:,40:490])\n",
        "print(dataset_bin[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P0gNXNIQTF0b"
      },
      "outputs": [],
      "source": [
        "# ID Split for binary classification\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_id, val_id, test_id = torch.utils.data.random_split(range(len(dataset_bin)), [0.7, 0.15, 0.15], generator=generator1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gftsWxpPTM9p",
        "outputId": "77594709-a758-4e51-c6b9-5f5fa3e5da4c"
      },
      "outputs": [],
      "source": [
        "# Dataset split for binary LSTM\n",
        "X_train = [dataset_bin[i] for i in train_id]\n",
        "X_val = [dataset_bin[i] for i in val_id]\n",
        "X_test = [dataset_bin[i] for i in test_id]\n",
        "y_train = np.array([labels_bin[i] for i in train_id])\n",
        "y_val = np.array([labels_bin[i] for i in val_id])\n",
        "y_test = np.array([labels_bin[i] for i in test_id])\n",
        "\n",
        "# Convert the list of torch tensors to a numpy array\n",
        "X_train = np.array([tensor.numpy() for tensor in X_train])\n",
        "X_val = np.array([tensor.numpy() for tensor in X_val])\n",
        "X_test = np.array([tensor.numpy() for tensor in X_test])\n",
        "\n",
        "print(len(X_train))\n",
        "print(X_train[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEU374WzTgLL",
        "outputId": "201cb39c-cab6-4a7c-f2a6-601c52fa4a51"
      },
      "outputs": [],
      "source": [
        "# Bi-LSTM model for binary classification\n",
        "input_shape = (128,450)\n",
        "input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "def bi_lstm_block(x, units, return_sequences=True):\n",
        "    x = Bidirectional(LSTM(units, return_sequences = return_sequences))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return x\n",
        "\n",
        "#LSTM blocks\n",
        "x = bi_lstm_block(input_layer, 128)\n",
        "x = bi_lstm_block(x, 128)\n",
        "x = bi_lstm_block(x, 128)\n",
        "x = bi_lstm_block(x, 128)\n",
        "x = bi_lstm_block(x, 128, return_sequences=False)\n",
        "\n",
        "\n",
        "# Dense blocks\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "output_layer = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs=input_layer, outputs = output_layer)\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dQOrrNOoTNnw"
      },
      "outputs": [],
      "source": [
        "# for saving metrica\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.train_losses.append(logs['loss'])\n",
        "        self.train_accuracies.append(logs['accuracy'])\n",
        "        self.val_losses.append(logs['val_loss'])\n",
        "        self.val_accuracies.append(logs['val_accuracy'])\n",
        "\n",
        "metrics_callback = MetricsCallback()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "aVK-PneBThlP",
        "outputId": "bdfce667-49d9-4bf8-fd18-f509a2e6e79c"
      },
      "outputs": [],
      "source": [
        "# Training of the Bi-LSTM model for 2 classes\n",
        "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=250,\n",
        "                    validation_data=(X_val, y_val), callbacks=[metrics_callback],verbose=1)\n",
        "\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# save training and validation metrics to text file\n",
        "with open('metrics.txt', 'w') as f:\n",
        "    f.write(\"Epoch\\tTrain Loss\\tTrain Accuracy\\tVal Loss\\tVal Accuracy\\n\")\n",
        "    for epoch, (train_loss, train_accuracy, val_loss, val_accuracy) in enumerate(zip(metrics_callback.train_losses,\n",
        "                                                                                      metrics_callback.train_accuracies,\n",
        "                                                                                      metrics_callback.val_losses,\n",
        "                                                                                      metrics_callback.val_accuracies)):\n",
        "        f.write(f\"{epoch+1}\\t{train_loss}\\t{train_accuracy}\\t{val_loss}\\t{val_accuracy}\\n\")\n",
        "\n",
        "# save test metrics\n",
        "with open('metrics.txt', 'a') as f:\n",
        "    f.write(f\"\\nTest Loss: {test_loss}\\n\")\n",
        "    f.write(f\"Test Accuracy: {test_accuracy}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9El5VudHToXA"
      },
      "source": [
        "CNN for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UPCilNu0Pt4P"
      },
      "outputs": [],
      "source": [
        "# Dataset split for binary CNN\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_id, val_id = torch.utils.data.random_split(range(len(dataset_bin)), [0.8, 0.2], generator=generator1)\n",
        "\n",
        "y_train = torch.tensor([labels_bin[i] for i in train_id])\n",
        "\n",
        "y_val = torch.tensor([labels_bin[i] for i in val_id])\n",
        "x_train = [dataset_bin[i] for i in train_id]\n",
        "x_train = torch.stack(x_train, dim = 0)\n",
        "x_val = [dataset_bin[i] for i in val_id]\n",
        "x_val = torch.stack(x_val, dim = 0)\n",
        "\n",
        "\n",
        "x_train = x_train.to(device=\"cuda\")\n",
        "x_val = x_val.to(device=\"cuda\")\n",
        "y_train = y_train.to(device='cuda')\n",
        "y_val = y_val.to(device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "wJDSph7DUAo_",
        "outputId": "965941e9-9564-4bae-9c59-6fcbc95f37a5"
      },
      "outputs": [],
      "source": [
        "# training CNN for binary classification\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.flatten_size = self._get_flatten_size()\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 1024, bias=True)\n",
        "        self.fc2 = nn.Linear(1024, 1, bias=True)\n",
        "\n",
        "    def _get_flatten_size(self):\n",
        "        x = torch.randn(1, 128, 450)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = self.pool(torch.relu(self.conv4(x)))\n",
        "        return x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "\n",
        "y_train = y_train.float().unsqueeze(1)\n",
        "y_val = y_val.float().unsqueeze(1)\n",
        "\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device='cuda')\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_labels in val_loader:\n",
        "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_labels).item() * val_inputs.size(0)\n",
        "            val_predicted = (val_outputs > 0.5).float()\n",
        "            val_total += val_labels.size(0)\n",
        "            val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    # Print results for the current epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl2024",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
